# 云原生技术公开课
# 1. 元原生概念
## 1.1 不可变基础设施
- 传统的发布：ssh链接机器后，上传/修改文件
- 云原生：容器的发布和替换，原容器不再使用
## 1.2 云应用编排理论
- 如何构建自包含、可定制的应用镜像
- 能不能实现应用的快速部署和隔离能力
- 应用基础设施创建和销毁的自动化管理
- 可复制的管控系统和支撑组件
# 2. 容器基本概念
## 2.1 定义
一组进程的集合
## 2.2 特点
### 2.2.1 视图隔离
针对不同进程使用同一个文件系统所造成的问题而言，Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离；进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程
### 2.2.2 资源可限制
因为进程之间相互可见并且可以相互通信，使用 Namespace 技术来实现进程在资源的视图上进行隔离。在 chroot 和 Namespace 的帮助下，进程就能够运行在一个独立的环境下了
### 2.2.3 独立文件系统
但在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过 Cgroup 来限制其资源使用率，设置其能够使用的 CPU 以及内存量
# 3. kubernetes核心概念
## 3.1 Pod
## 3.2 Volume
## 3.3 Deployment
## 3.4 Service
## 3.5 Namespace
# 4. Pod和容器设计模式
## 4.1 Pod
## 4.2 容器设计模式：sidecar
# 5. 应用编排和管理核心原理
## 5.1 kubernetes资源对象
- spec：期望的状态
- status：观测到的状态
- labels：识别资源的标签
- annotations：资源的注解
- owenReference：多个资源之间的相互关系
## 5.2 控制器模式
声明式API，而不是命令式API
# 6. Deployment
## 6.1 背景
pod管理面临的问题：
- 如何保证集群中pod可用的数量
- 如何为所有pod更新镜像版本
- 更新过程中，如果保证服务可用性
- 如何快速回滚
## 6.2 架构设计
Deployment只管理不同版本的ReplicaSet，由ReplicaSet来管理具体的Pod副本数，每个ReplicaSet对应Deployment template的一个版本。
# 7. Job和DaemonSet
## 7.2 Job
### 7.2.1 背景
- 我们如何保证 Pod 内进程正确的结束？
- 如何保证进程运行失败后重试？
- 如何管理多个任务，且任务之间有依赖关系？
- 如何并行地运行任务，并管理任务的队列大小？
## 7.4 DaemonSet
### 7.4.1 背景
- 首先如果希望每个节点都运行同样一个 pod 怎么办？
- 如果新节点加入集群的时候，想要立刻感知到它，然后去部署一个 pod，帮助我们初始化一些东西，这个需求如何做？
- 如果有节点退出的时候，希望对应的 pod 会被删除掉，应该怎么操作？
- 如果 pod 状态异常的时候，我们需要及时地监控这个节点异常，然后做一些监控或者汇报的一些动作，那么这些东西运用什么控制器来做？
# 8. 应用配置管理
## 8.1 背景
用一个容器镜像来启动一个 container，其实有很多需要配套的问题待解决：
- 比如说一些可变的配置。因为我们不可能把一些可变的配置写到镜像里面，当这个配置需要变化的时候，可能需要我们重新编译一次镜像，这个肯定是不能接受的；
- 一些敏感信息的存储和使用。比如说应用需要使用一些密码，或者用一些 token；
- 我们容器要访问集群自身。比如我要访问 kube-apiserver，那么本身就有一个身份认证的问题；
- 容器在节点上运行之后，它的资源需求；
- 容器在节点上，它们是共享内核的，那么它的一个安全管控怎么办？
- 容器启动之前的一个前置条件检验。比如说，一个容器启动之前，我可能要确认一下 DNS 服务是不是好用？又或者确认一下网络是不是联通的？那么这些其实就是一些前置的校验。
## 8.2 ConfigMap
## 8.3 Secret
## 8.4 SecretAccount
## 8.5 Resources
## 8.6 SecurityContext
## 8.7 InitContainers
# 9. 应用存储和持久化数据卷
## 9.1 Volumes介绍
### 9.1.1 Persistent Volumes
### 9.1.2 Persistent Volumes Claim
- Static Volume Provisioning
- Dynamic Volume Provisioning
# 10. 应用存储和持久化数据卷：应用快照和拓扑调度
## 10.1 快照背景
在使用存储时，为了提高数据操作的容错性，我们通常有需要对线上数据进行snapshot，以及能快速restore的能力。另外，当需要对线上数据进行快速的复制以及迁移等动作，如进行环境的复制、数据开发等功能时，都可以通过存储快照来满足需求，而 K8s 中通过 CSI Snapshotter controller 来实现存储快照的功能。
## 10.2 snapshot
## 10.3 restore
## 10.4 topology
拓扑是 K8s 集群中为管理的 nodes 划分的一种“位置”关系，意思为：可以通过在 node 的 labels 信息里面填写某一个 node 属于某一个拓扑。
- 地区region
- 可用区available zone
- 单机维度hostname
# 11. 可观测性
## 11.1 背景
首先来看一下，整个需求的来源：当把应用迁移到 Kubernetes 之后，要如何去保障应用的健康与稳定呢？其实很简单，可以从两个方面来进行增强：
- 提高应用的可观测性；
- 提高应用的可恢复能力。
从可观测性上来讲，可以在三个方面来去做增强：
- 应用的健康状态上面，可以实时地进行观测；
- 获取应用的资源使用情况；
- 拿到应用的实时日志，进行问题的诊断与分析。
当出现了问题之后，首先要做的事情是要降低影响的范围，进行问题的调试与诊断。最后当出现问题的时候，理想的状况是：可以通过和 K8s 集成的自愈机制进行完整的恢复。
## 11.2 Liveness 与 Readiness
探测方式：
- httpGet。它是通过发送 http Get 请求来进行判断的，当返回码是 200-399 之间的状态码时，标识这个应用是健康的；
- Exec。它是通过执行容器中的一个命令来判断当前的服务是否是正常的，当命令行的返回结果是 0，则标识容器是健康的；
- tcpSocket。它是通过探测容器的 IP 和 Port 进行 TCP 健康检查，如果这个 TCP 的链接能够正常被建立，那么标识当前这个容器是健康的。
探测结果：
- 第一种是 success，当状态是 success 的时候，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态； 
- 第二种是 Failure，Failure 表示的是这个 container 没有通过健康检查，如果没有通过健康检查的话，那么此时就会进行相应的一个处理，那在 Readiness 处理的一个方式就是通过 service。service 层将没有通过 Readiness 的 pod 进行摘除，而 Liveness 就是将这个 pod 进行重新拉起，或者是删除。 
- 第三种状态是 Unknown，Unknown 是表示说当前的执行的机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，那么此时 Readiness-probe 或 Liveness-probe 会不做任何的一个操作，会等待下一次的机制来进行检验。
适用场景：
- Liveness 指针适用场景是支持那些可以重新拉起的应用，而 Readiness 指针主要应对的是启动之后无法立即对外提供服务的这些应用。
## 11.3 问题诊断
K8s 是整个的一个设计是面向状态机的，它里面通过 yaml 的方式来定义的是一个期望到达的一个状态，而真正这个 yaml 在执行过程中会由各种各样的 controller来负责整体的状态之间的一个转换。
### 11.3.1 Pod 停留在 Pending
### 11.3.2 Pod 停留在 waiting
### 11.3.3 Pod 不断被拉取并且可以看到 crashing
### 11.3.4 Pod 处在 Runing 但是没有正常工作
### 11.3.5 Service 无法正常的工作
## 11.4 应用远程调试
Pod 远程调试

Service 远程调试

调试工具 - kubectl-debug
# 12. 可观测性：监控与日志
监控和日志是大型分布式系统的重要基础设施，监控可以帮助开发者查看系统的运行状态，而日志可以协助问题的排查和诊断。
## 12.1 监控
- 资源监控：比较常见的像 CPU、内存、网络这种资源类的一个指标
- 性能监控：性能监控指的就是 APM 监控，也就是说常见的一些应用性能类的监控指标的检查。通常是通过一些 Hook 的机制在虚拟机层、字节码执行层通过隐式调用，或者是在应用层显示注入，获取更深层次的一个监控指标，一般是用来应用的调优和诊断的。
- 安全监控：类似像越权管理、安全漏洞扫描等等
- 事件监控：K8s 中的一个设计理念，就是基于状态机的一个状态转换。从正常的状态转换成另一个正常的状态的时候，会发生一个 normal 的事件，而从一个正常状态转换成一个异常状态的时候，会发生一个 warning 的事件。事件监控就是可以把 normal 的事件或者是 warning 事件离线到一个数据中心，然后通过数据中心的分析以及报警
监控演进：Heapster->metrics-server
### 12.1.1 Kubernetes 的监控接口标准
Resource Metrice
Custom Metrics
External Metrics
Promethues - 开源社区的监控“标准”
## 12.2 日志
### 12.2.1 分类
- 主机内核的日志
- Runtime 的日志
- 核心组件的日志
- 部署应用的日志
### 12.2.2 采集
- 宿主机文件
- 容器内有日志文件
- 直接写到 stdout
# 13. kubernetes网络策略及策略控制
## 13.1  Kubernetes的容器网络模型
### 13.1.1 约法三章
- 任意两个 pod 之间其实是可以直接通信的，无需经过显式地使用 NAT 来接收数据和地址的转换；
- node 与 pod 之间是可以直接通信的，无需使用明显的地址转换；
- pod 看到自己的 IP 跟别人看见它所用的IP是一样的，中间不能经过转换。
### 13.1.1 四大目标
- 外部世界和 service 之间是怎么通信的
- service 如何与它后端的 pod 通讯
- pod 和 pod 之间调用是怎么做到通信的
- pod 内部容器与容器之间的通信

容器网络方案大体分为 Underlay/Overlay 两大派别：
- Underlay 的标准是它与 Host 网络是同层的，从外在可见的一个特征就是它是不是使用了 Host 网络同样的网段、输入输出基础设备、容器的 IP 地址是不是需要与 Host 网络取得协同（来自同一个中心分配或统一划分）。这就是 Underlay；
- Overlay 不一样的地方就在于它并不需要从 Host 网络的 IPM 的管理的组件去申请IP，一般来说，它只需要跟 Host 网络不冲突，这个 IP 可以自由分配的。
## 13.2 netns探秘
## 13.3 主流网络方案
- Flannel：用户态的udp，内核的Vxlan，host-gw
- Calico
- Canal
- WeaveNet
## 13.4 network policy
# 14. kubernetes service
# 15. 深入剖析Linux容器
# 16. 深入理解etcd：基本原理解析
# 17. 深入理解etcd：etcd性能优化实践
# 18. kubernetes调度和资源管理
# 19. 调度器的调度流程和算法介绍
# 20. GPU管理和Device Plugin工作机制
# 21. kubernetes存储架构及插件使用
# 22. 有状态应用编排：StatefulSet
# 23. kubernetesAPI编程范式
# 24. kubernetesAPI变成利器：Operator和OperatorFramework
# 25. kubernetes网络模型进阶
# 26. 理解CNI和CNI插件
# 27. kubernetes安全之访问控制
# 28. 理解容器运行时接口CRI
# 29. 安全容器技术
# 30. 理解RuntimeClass与使用多容器运行时
